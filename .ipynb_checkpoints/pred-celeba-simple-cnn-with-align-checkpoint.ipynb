{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from os.path import join\n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_HOME = '../celeba/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('celeba.aligned.train.csv', nrows=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(dataset['person'])\n",
    "dataset['person_id'] = encoder.transform(dataset['person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>count</th>\n",
       "      <th>person</th>\n",
       "      <th>person_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>069803.png</td>\n",
       "      <td>20</td>\n",
       "      <td>4386</td>\n",
       "      <td>4385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>182960.png</td>\n",
       "      <td>29</td>\n",
       "      <td>7325</td>\n",
       "      <td>7324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>187891.png</td>\n",
       "      <td>9</td>\n",
       "      <td>6868</td>\n",
       "      <td>6867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>142184.png</td>\n",
       "      <td>26</td>\n",
       "      <td>3213</td>\n",
       "      <td>3212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>064549.png</td>\n",
       "      <td>15</td>\n",
       "      <td>1904</td>\n",
       "      <td>1903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         path  count  person  person_id\n",
       "0  069803.png     20    4386       4385\n",
       "1  182960.png     29    7325       7324\n",
       "2  187891.png      9    6868       6867\n",
       "3  142184.png     26    3213       3212\n",
       "4  064549.png     15    1904       1903"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.87628967279159"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dataset.groupby('person_id')['person'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if the count > 30, will random pick 30 + (count-30)*0.2 imgs for the person\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "FACE_NUM = 45\n",
    "\n",
    "def crit(idx):\n",
    "    row = dataset.ix[idx]\n",
    "    count = row['count']\n",
    "    if count < FACE_NUM: return True\n",
    "    ratio = (FACE_NUM+(count-FACE_NUM)*0.4) / count\n",
    "    return random.random() < ratio\n",
    "\n",
    "def crit2(idx):\n",
    "    row = dataset.ix[idx]\n",
    "    count = row['count']\n",
    "    if count < 7: return True\n",
    "    return False\n",
    "    \n",
    "# dataset = dataset.select(crit)\n",
    "dataset = pd.concat([dataset, dataset.select(crit2)], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.3327110150339"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dataset.groupby('person_id')['person'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = dataset['person_id'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413852,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate([y,y])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206926\n"
     ]
    }
   ],
   "source": [
    "img_paths = [r.path for r in dataset.itertuples()]\n",
    "print(len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'069803.png'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def path2ImgVec(path):\n",
    "#     x = img_to_array(load_img(join(DATA_HOME, path)).convert('L'))\n",
    "    x = img_to_array(load_img(join(DATA_HOME, path)))\n",
    "    return x.reshape((1,) + x.shape)\n",
    "\n",
    "def path2ImgVecFlipped(path):\n",
    "#     img = load_img(join(DATA_HOME, path)).convert('L')\n",
    "    img = load_img(join(DATA_HOME, path))\n",
    "    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    x = img_to_array(img)\n",
    "    return x.reshape((1,) + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC8AAAA3CAIAAAATwIQNAAAVhklEQVR4nDV4V69m2Y0dyR1O/M4X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\n/f73N+9/yupmdzwCoUiKov4fB0taVjhAy7sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=47x55 at 0x7F2CA0A05668>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_img(join(DATA_HOME, img_paths[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(8)\n",
    "results1 = pool.map(path2ImgVec, img_paths)\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(8)\n",
    "results2 = pool.map(path2ImgVecFlipped, img_paths)\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.vstack(results1+results2)\n",
    "# X = np.vstack(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413852, 55, 47, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(results1)): \n",
    "    results1[i] = None\n",
    "del results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(results2)): \n",
    "    results2[i] = None\n",
    "del results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_class 10177\n"
     ]
    }
   ],
   "source": [
    "nb_class = len(np.unique(y))\n",
    "print('nb_class', nb_class)\n",
    "hidden_dim = 160\n",
    "best_weights_filepath = '../models/best_weights.hdf5'\n",
    "input_shape = X.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import LSTM\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import metrics\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.engine import Model\n",
    "from keras import optimizers\n",
    "\n",
    "def build_model():\n",
    "    image_input = Input(shape=input_shape)\n",
    "    \n",
    "    conv1 = Conv2D(20, (4, 4), name='conv1')(image_input)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2), name='pool1')(conv1)\n",
    "#     pool1 = Dropout(rate=0.2)(pool1)\n",
    "    \n",
    "    conv2 = Conv2D(40, (3, 3), name='conv2')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2), name='pool2')(conv2)\n",
    "#     pool2 = Dropout(rate=0.2)(pool2)\n",
    "\n",
    "    conv3 = Conv2D(60, (3, 3), name='conv3')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2), name='pool3')(conv3)\n",
    "\n",
    "    flat1 = Flatten(name='flat1')(pool3)\n",
    "    \n",
    "    conv4 = Conv2D(80, (2, 2), name='conv4')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    flat2 = Flatten(name='flat2')(conv4)\n",
    "    \n",
    "    merged = concatenate([flat1, flat2])\n",
    "    \n",
    "    out = Dense(hidden_dim, name='hidden1')(merged)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu',name='deepid')(out)\n",
    "    out = Dense(nb_class, activation='softmax', name='softmax_class')(out)\n",
    "    \n",
    "    model = Model(inputs=image_input, outputs=out)\n",
    "\n",
    "    # optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0001)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam', #rmsprop\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 55, 47, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 52, 44, 20)    980                                          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 52, 44, 20)    80                                           \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 52, 44, 20)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)             (None, 26, 22, 20)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                   (None, 24, 20, 40)    7240                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 24, 20, 40)    160                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 24, 20, 40)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)             (None, 12, 10, 40)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                   (None, 10, 8, 60)     21660                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 10, 8, 60)     240                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 10, 8, 60)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)             (None, 5, 4, 60)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                   (None, 4, 3, 80)      19280                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 4, 3, 80)      320                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 4, 3, 80)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flat1 (Flatten)                  (None, 1200)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flat2 (Flatten)                  (None, 960)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 2160)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden1 (Dense)                  (None, 160)           345760                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 160)           640                                          \n",
      "____________________________________________________________________________________________________\n",
      "deepid (Activation)              (None, 160)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "softmax_class (Dense)            (None, 10177)         1638497                                      \n",
      "====================================================================================================\n",
      "Total params: 2,034,857.0\n",
      "Trainable params: 2,034,137.0\n",
      "Non-trainable params: 720.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 335219 samples, validate on 37247 samples\n",
      "Epoch 1/30\n",
      "49s - loss: 7.7860 - acc: 0.0218 - val_loss: 7.0255 - val_acc: 0.0320\n",
      "Epoch 2/30\n",
      "48s - loss: 5.0315 - acc: 0.2128 - val_loss: 4.3340 - val_acc: 0.2739\n",
      "Epoch 3/30\n",
      "48s - loss: 3.1170 - acc: 0.4656 - val_loss: 3.2664 - val_acc: 0.4147\n",
      "Epoch 4/30\n",
      "48s - loss: 2.0939 - acc: 0.6197 - val_loss: 2.7073 - val_acc: 0.5064\n",
      "Epoch 5/30\n",
      "48s - loss: 1.5234 - acc: 0.7148 - val_loss: 2.5073 - val_acc: 0.5295\n",
      "Epoch 6/30\n",
      "49s - loss: 1.1564 - acc: 0.7800 - val_loss: 2.2742 - val_acc: 0.5689\n",
      "Epoch 7/30\n",
      "49s - loss: 0.8997 - acc: 0.8274 - val_loss: 2.0706 - val_acc: 0.6074\n",
      "Epoch 8/30\n",
      "49s - loss: 0.7052 - acc: 0.8644 - val_loss: 1.9955 - val_acc: 0.6163\n",
      "Epoch 9/30\n",
      "49s - loss: 0.5546 - acc: 0.8942 - val_loss: 1.9747 - val_acc: 0.6137\n",
      "Epoch 10/30\n",
      "49s - loss: 0.4400 - acc: 0.9171 - val_loss: 1.9539 - val_acc: 0.6213\n",
      "Epoch 11/30\n",
      "49s - loss: 0.3466 - acc: 0.9364 - val_loss: 1.8604 - val_acc: 0.6394\n",
      "Epoch 12/30\n",
      "49s - loss: 0.2757 - acc: 0.9510 - val_loss: 2.0133 - val_acc: 0.6167\n",
      "Epoch 13/30\n",
      "50s - loss: 0.2192 - acc: 0.9619 - val_loss: 1.7906 - val_acc: 0.6600\n",
      "Epoch 14/30\n",
      "51s - loss: 0.1748 - acc: 0.9712 - val_loss: 2.1039 - val_acc: 0.6092\n",
      "Epoch 15/30\n",
      "51s - loss: 0.1430 - acc: 0.9770 - val_loss: 1.8781 - val_acc: 0.6498\n",
      "Epoch 16/30\n",
      "52s - loss: 0.1173 - acc: 0.9821 - val_loss: 1.8417 - val_acc: 0.6623\n",
      "Epoch 17/30\n",
      "52s - loss: 0.0873 - acc: 0.9871 - val_loss: 2.5358 - val_acc: 0.5592\n",
      "Epoch 19/30\n",
      "52s - loss: 0.0774 - acc: 0.9884 - val_loss: 2.0160 - val_acc: 0.6408\n",
      "Epoch 20/30\n",
      "52s - loss: 0.0806 - acc: 0.9861 - val_loss: 2.1007 - val_acc: 0.6416\n",
      "Epoch 21/30\n",
      "52s - loss: 0.0858 - acc: 0.9834 - val_loss: 2.2262 - val_acc: 0.6205\n",
      "Epoch 22/30\n",
      "52s - loss: 0.0895 - acc: 0.9808 - val_loss: 2.3390 - val_acc: 0.6106\n",
      "Epoch 23/30\n",
      "52s - loss: 0.0787 - acc: 0.9832 - val_loss: 2.2992 - val_acc: 0.6173\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-444b7a7b66f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msaveBestModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearlyStopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;32m/home/liyin/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liyin/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liyin/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liyin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liyin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liyin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/liyin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liyin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saveBestModel = keras.callbacks.ModelCheckpoint(\n",
    "    best_weights_filepath, \n",
    "    monitor='val_loss', \n",
    "    verbose=0, \n",
    "    save_best_only=True, \n",
    "    mode='auto'\n",
    ")\n",
    "earlyStopping=keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    verbose=1, \n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    batch_size=1024, \n",
    "    epochs=30,\n",
    "    verbose=2, \n",
    "    validation_split=0.1, \n",
    "    shuffle=True,\n",
    "    callbacks=[saveBestModel, earlyStopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(best_weights_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../models/celeba-full-simple-cnn.aligned.margin32.flipped.b1024.s30.model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=1024, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
