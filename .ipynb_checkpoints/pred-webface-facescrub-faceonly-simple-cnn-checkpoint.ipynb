{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from os.path import join\n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_HOME1 = '../webface-aligned-with-32-margin-resized/'\n",
    "DATA_HOME2 = '../faceonly-aligned-with-32-margin-resized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65331 entries, 0 to 65330\n",
      "Data columns (total 3 columns):\n",
      "person    65331 non-null object\n",
      "count     65331 non-null int64\n",
      "path      65331 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset1 = pd.read_csv('webface.aligned.train.csv', nrows=None)\n",
    "dataset1['path'] = dataset1['path'].map(lambda x: join(DATA_HOME1, x))\n",
    "dataset1['person'] = dataset1['person'].map(str)\n",
    "dataset1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51959 entries, 0 to 51958\n",
      "Data columns (total 3 columns):\n",
      "person    51959 non-null object\n",
      "count     51959 non-null int64\n",
      "path      51959 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset2 = pd.read_csv('facescrub-faceonly.aligned.train.csv', nrows=None)\n",
    "dataset2['path'] = dataset2['path'].map(lambda x: join(DATA_HOME2, x))\n",
    "dataset2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset1, dataset2], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117290 entries, 0 to 117289\n",
      "Data columns (total 4 columns):\n",
      "index     117290 non-null int64\n",
      "person    117290 non-null object\n",
      "count     117290 non-null int64\n",
      "path      117290 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(dataset['person'])\n",
    "dataset['person_id'] = encoder.transform(dataset['person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.180209171359614"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dataset.groupby('person_id')['person'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if the count > 50, will random pick 50 + (count-50)*0.25 imgs for the person\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def crit(idx):\n",
    "    row = dataset.ix[idx]\n",
    "    count = row['count']\n",
    "    if count < 50: return True\n",
    "    ratio = (50+(count-50)*0.75) / count\n",
    "    return random.random() < ratio\n",
    "\n",
    "def crit2(idx):\n",
    "    row = dataset.ix[idx]\n",
    "    count = row['count']\n",
    "    if count < 6: return True\n",
    "    return False\n",
    "    \n",
    "# dataset = dataset.select(crit)\n",
    "dataset = pd.concat([dataset, dataset.select(crit2)], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>person</th>\n",
       "      <th>count</th>\n",
       "      <th>path</th>\n",
       "      <th>person_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3331486</td>\n",
       "      <td>13</td>\n",
       "      <td>../webface-aligned-with-32-margin-resized/3331...</td>\n",
       "      <td>1134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3331486</td>\n",
       "      <td>13</td>\n",
       "      <td>../webface-aligned-with-32-margin-resized/3331...</td>\n",
       "      <td>1134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3331486</td>\n",
       "      <td>13</td>\n",
       "      <td>../webface-aligned-with-32-margin-resized/3331...</td>\n",
       "      <td>1134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3331486</td>\n",
       "      <td>13</td>\n",
       "      <td>../webface-aligned-with-32-margin-resized/3331...</td>\n",
       "      <td>1134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3331486</td>\n",
       "      <td>13</td>\n",
       "      <td>../webface-aligned-with-32-margin-resized/3331...</td>\n",
       "      <td>1134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index   person  count  \\\n",
       "0        0      0  3331486     13   \n",
       "1        1      1  3331486     13   \n",
       "2        2      2  3331486     13   \n",
       "3        3      3  3331486     13   \n",
       "4        4      4  3331486     13   \n",
       "\n",
       "                                                path  person_id  \n",
       "0  ../webface-aligned-with-32-margin-resized/3331...       1134  \n",
       "1  ../webface-aligned-with-32-margin-resized/3331...       1134  \n",
       "2  ../webface-aligned-with-32-margin-resized/3331...       1134  \n",
       "3  ../webface-aligned-with-32-margin-resized/3331...       1134  \n",
       "4  ../webface-aligned-with-32-margin-resized/3331...       1134  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.19991954947707"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dataset.groupby('person_id')['person'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = dataset['person_id'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234678,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate([y,y])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117339\n"
     ]
    }
   ],
   "source": [
    "img_paths = [r.path for r in dataset.itertuples()]\n",
    "print(len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC8AAAA3CAIAAAATwIQNAAAUMklEQVR4nCWXSY8k2ZWd7/TeMzN3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\n9HQ1pMP2nt1i1wtzIrHaTvzs/wMluPMN/j0yYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=47x55 at 0x7FA5F2E5F128>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "load_img(img_paths[0]).transpose(Image.FLIP_LEFT_RIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def path2ImgVec(path):\n",
    "    x = img_to_array(load_img(path))\n",
    "    return x.reshape((1,) + x.shape)\n",
    "\n",
    "def path2ImgVecFlipped(path):\n",
    "    img = load_img(path)\n",
    "    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    x = img_to_array(img)\n",
    "    return x.reshape((1,) + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(8)\n",
    "results1 = pool.map(path2ImgVec, img_paths)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(8)\n",
    "results2 = pool.map(path2ImgVecFlipped, img_paths)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.vstack(results1+results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234678, 55, 47, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_class 2486\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import LSTM\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import metrics\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.engine import Model\n",
    "from keras import optimizers\n",
    "\n",
    "nb_class = len(np.unique(y))\n",
    "print('nb_class', nb_class)\n",
    "hidden_dim = 160\n",
    "best_weights_filepath = '../models/webface-facescrub-faceonly.best_weights.hdf5'\n",
    "\n",
    "def build_model():\n",
    "    image_input = Input(shape=X.shape[1:])\n",
    "    \n",
    "    conv1 = Conv2D(20, (4, 4), name='conv1')(image_input)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2), name='pool1')(conv1)\n",
    "#     pool1 = Dropout(rate=0.2)(pool1)\n",
    "    \n",
    "    conv2 = Conv2D(40, (3, 3), name='conv2')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2), name='pool2')(conv2)\n",
    "#     pool2 = Dropout(rate=0.2)(pool2)\n",
    "\n",
    "    conv3 = Conv2D(60, (3, 3), name='conv3')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2), name='pool3')(conv3)\n",
    "\n",
    "    flat1 = Flatten(name='flat1')(pool3)\n",
    "    \n",
    "    conv4 = Conv2D(80, (2, 2), name='conv4')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    flat2 = Flatten(name='flat2')(conv4)\n",
    "    \n",
    "    merged = concatenate([flat1, flat2])\n",
    "    \n",
    "    out = Dense(hidden_dim, name='hidden1')(merged)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu',name='deepid')(out)\n",
    "    out = Dense(nb_class, activation='softmax', name='softmax_class')(out)\n",
    "    \n",
    "    model = Model(inputs=image_input, outputs=out)\n",
    "\n",
    "#     optimizer = optimizers.Adam(lr=1e-2, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=(1e-4))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam', #rmsprop\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 55, 47, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 52, 44, 20)    980                                          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 52, 44, 20)    80                                           \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 52, 44, 20)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)             (None, 26, 22, 20)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                   (None, 24, 20, 40)    7240                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 24, 20, 40)    160                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 24, 20, 40)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)             (None, 12, 10, 40)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                   (None, 10, 8, 60)     21660                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 10, 8, 60)     240                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 10, 8, 60)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)             (None, 5, 4, 60)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                   (None, 4, 3, 80)      19280                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 4, 3, 80)      320                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 4, 3, 80)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flat1 (Flatten)                  (None, 1200)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flat2 (Flatten)                  (None, 960)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 2160)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden1 (Dense)                  (None, 160)           345760                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 160)           640                                          \n",
      "____________________________________________________________________________________________________\n",
      "deepid (Activation)              (None, 160)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "softmax_class (Dense)            (None, 2486)          400246                                       \n",
      "====================================================================================================\n",
      "Total params: 796,606.0\n",
      "Trainable params: 795,886.0\n",
      "Non-trainable params: 720.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 190089 samples, validate on 21121 samples\n",
      "Epoch 1/12\n",
      "32s - loss: 5.8292 - acc: 0.1170 - val_loss: 4.9865 - val_acc: 0.1439\n",
      "Epoch 2/12\n",
      "31s - loss: 3.3912 - acc: 0.4043 - val_loss: 3.0924 - val_acc: 0.4257\n",
      "Epoch 3/12\n",
      "31s - loss: 2.2266 - acc: 0.5786 - val_loss: 2.6905 - val_acc: 0.4839\n",
      "Epoch 4/12\n",
      "31s - loss: 1.6615 - acc: 0.6725 - val_loss: 2.2302 - val_acc: 0.5715\n",
      "Epoch 5/12\n",
      "31s - loss: 1.3154 - acc: 0.7344 - val_loss: 2.5817 - val_acc: 0.5062\n",
      "Epoch 6/12\n",
      "31s - loss: 1.0694 - acc: 0.7789 - val_loss: 2.1330 - val_acc: 0.5825\n",
      "Epoch 7/12\n",
      "32s - loss: 0.8746 - acc: 0.8162 - val_loss: 1.9645 - val_acc: 0.6139\n",
      "Epoch 8/12\n",
      "32s - loss: 0.7203 - acc: 0.8461 - val_loss: 1.9435 - val_acc: 0.6210\n",
      "Epoch 9/12\n",
      "32s - loss: 0.5945 - acc: 0.8715 - val_loss: 1.9692 - val_acc: 0.6179\n",
      "Epoch 10/12\n",
      "32s - loss: 0.4915 - acc: 0.8928 - val_loss: 2.0475 - val_acc: 0.6104\n",
      "Epoch 11/12\n",
      "32s - loss: 0.4058 - acc: 0.9113 - val_loss: 2.0224 - val_acc: 0.6190\n",
      "Epoch 12/12\n",
      "33s - loss: 0.3310 - acc: 0.9281 - val_loss: 2.0628 - val_acc: 0.6191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5e124ceb8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveBestModel = keras.callbacks.ModelCheckpoint(\n",
    "    best_weights_filepath, \n",
    "    monitor='val_loss', \n",
    "    verbose=0, \n",
    "    save_best_only=True, \n",
    "    mode='auto'\n",
    ")\n",
    "earlyStopping=keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    verbose=1, \n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    batch_size=512, \n",
    "    epochs=10,\n",
    "    verbose=2, \n",
    "    validation_split=0.1, \n",
    "    shuffle=True,\n",
    "    callbacks=[saveBestModel, earlyStopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(best_weights_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../models/webface-facescrub-faceonly-simple-cnn.aligned.flipped.model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.9424724528467268, 0.62182546338536004]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
