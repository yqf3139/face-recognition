{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from os.path import join\n",
    "import multiprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_HOME1 = '/ext_drive/liyin/webface-full-aligned-with-42-margin-resized/'\n",
    "DATA_HOME2 = '/ext_drive/liyin/celeba-aligned-with-42-margin-resized/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset1 = pd.read_csv('webface-full.aligned.train.csv', nrows=None)\n",
    "dataset2 = pd.read_csv('celeba.aligned.train.csv', nrows=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset1['path'] = dataset1['path'].apply(lambda x: join(DATA_HOME1, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset2['path'] = dataset2['path'].apply(lambda x: join(DATA_HOME2, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>count</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3331486</td>\n",
       "      <td>12</td>\n",
       "      <td>/ext_drive/liyin/webface-full-aligned-with-42-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3331486</td>\n",
       "      <td>12</td>\n",
       "      <td>/ext_drive/liyin/webface-full-aligned-with-42-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3331486</td>\n",
       "      <td>12</td>\n",
       "      <td>/ext_drive/liyin/webface-full-aligned-with-42-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3331486</td>\n",
       "      <td>12</td>\n",
       "      <td>/ext_drive/liyin/webface-full-aligned-with-42-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3331486</td>\n",
       "      <td>12</td>\n",
       "      <td>/ext_drive/liyin/webface-full-aligned-with-42-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    person  count                                               path\n",
       "0  3331486     12  /ext_drive/liyin/webface-full-aligned-with-42-...\n",
       "1  3331486     12  /ext_drive/liyin/webface-full-aligned-with-42-...\n",
       "2  3331486     12  /ext_drive/liyin/webface-full-aligned-with-42-...\n",
       "3  3331486     12  /ext_drive/liyin/webface-full-aligned-with-42-...\n",
       "4  3331486     12  /ext_drive/liyin/webface-full-aligned-with-42-..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>count</th>\n",
       "      <th>person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/ext_drive/liyin/celeba-aligned-with-42-margin...</td>\n",
       "      <td>20</td>\n",
       "      <td>4386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/ext_drive/liyin/celeba-aligned-with-42-margin...</td>\n",
       "      <td>29</td>\n",
       "      <td>7325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/ext_drive/liyin/celeba-aligned-with-42-margin...</td>\n",
       "      <td>9</td>\n",
       "      <td>6868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/ext_drive/liyin/celeba-aligned-with-42-margin...</td>\n",
       "      <td>26</td>\n",
       "      <td>3213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/ext_drive/liyin/celeba-aligned-with-42-margin...</td>\n",
       "      <td>15</td>\n",
       "      <td>1904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  count  person\n",
       "0  /ext_drive/liyin/celeba-aligned-with-42-margin...     20    4386\n",
       "1  /ext_drive/liyin/celeba-aligned-with-42-margin...     29    7325\n",
       "2  /ext_drive/liyin/celeba-aligned-with-42-margin...      9    6868\n",
       "3  /ext_drive/liyin/celeba-aligned-with-42-margin...     26    3213\n",
       "4  /ext_drive/liyin/celeba-aligned-with-42-margin...     15    1904"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset1, dataset2], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(dataset['person'])\n",
    "dataset['person_id'] = encoder.transform(dataset['person'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "      <th>path</th>\n",
       "      <th>person</th>\n",
       "      <th>person_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>/ext_drive/liyin/webface-full-aligned-with-42-...</td>\n",
       "      <td>3331486</td>\n",
       "      <td>18508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>/ext_drive/liyin/webface-full-aligned-with-42-...</td>\n",
       "      <td>3331486</td>\n",
       "      <td>18508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>/ext_drive/liyin/webface-full-aligned-with-42-...</td>\n",
       "      <td>3331486</td>\n",
       "      <td>18508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>/ext_drive/liyin/webface-full-aligned-with-42-...</td>\n",
       "      <td>3331486</td>\n",
       "      <td>18508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>/ext_drive/liyin/webface-full-aligned-with-42-...</td>\n",
       "      <td>3331486</td>\n",
       "      <td>18508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  count                                               path   person  \\\n",
       "0      0     12  /ext_drive/liyin/webface-full-aligned-with-42-...  3331486   \n",
       "1      1     12  /ext_drive/liyin/webface-full-aligned-with-42-...  3331486   \n",
       "2      2     12  /ext_drive/liyin/webface-full-aligned-with-42-...  3331486   \n",
       "3      3     12  /ext_drive/liyin/webface-full-aligned-with-42-...  3331486   \n",
       "4      4     12  /ext_drive/liyin/webface-full-aligned-with-42-...  3331486   \n",
       "\n",
       "   person_id  \n",
       "0      18508  \n",
       "1      18508  \n",
       "2      18508  \n",
       "3      18508  \n",
       "4      18508  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 655695 entries, 0 to 655694\n",
      "Data columns (total 5 columns):\n",
      "index        655695 non-null int64\n",
      "count        655695 non-null int64\n",
      "path         655695 non-null object\n",
      "person       655695 non-null int64\n",
      "person_id    655695 non-null int64\n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 25.0+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.741316317604074"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dataset.groupby('person_id')['person'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if the count > 30, will random pick 30 + (count-30)*0.2 imgs for the person\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "FACE_NUM = 45\n",
    "\n",
    "def crit(idx):\n",
    "    row = dataset.ix[idx]\n",
    "    count = row['count']\n",
    "    if count < FACE_NUM: return True\n",
    "    ratio = (FACE_NUM+(count-FACE_NUM)*0.4) / count\n",
    "    return random.random() < ratio\n",
    "\n",
    "def crit2(idx):\n",
    "    row = dataset.ix[idx]\n",
    "    count = row['count']\n",
    "    if count < 6: return True\n",
    "    return False\n",
    "    \n",
    "# dataset = dataset.select(crit)\n",
    "dataset = pd.concat([dataset, dataset.select(crit2)], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.91946688622446"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dataset.groupby('person_id')['person'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = dataset['person_id'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318314,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate([y,y])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659157\n"
     ]
    }
   ],
   "source": [
    "img_paths = [r.path for r in dataset.itertuples()]\n",
    "print(len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ext_drive/liyin/webface-full-aligned-with-42-margin-resized/3331486/007.png'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def path2ImgVec(path):\n",
    "    x = img_to_array(load_img(path).convert('L'))\n",
    "    return x.reshape((1,) + x.shape)\n",
    "\n",
    "def path2ImgVecFlipped(path):\n",
    "    img = load_img(path).convert('L')\n",
    "    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    x = img_to_array(img)\n",
    "    return x.reshape((1,) + x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC8AAAA3CAAAAAC5yUyGAAAHI0lEQVR4nAXBWW9dVxUA4DXtfYZ7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\nQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=47x55 at 0x7F8986E0BA58>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_img(img_paths[0]).convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(8)\n",
    "results1 = pool.map(path2ImgVec, img_paths)\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(8)\n",
    "results2 = pool.map(path2ImgVecFlipped, img_paths)\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.vstack(results1+results2)\n",
    "# X = np.vstack(results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318314, 55, 47, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(results1)): \n",
    "    results1[i] = None\n",
    "del results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(results2)): \n",
    "    results2[i] = None\n",
    "del results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_class 19433\n"
     ]
    }
   ],
   "source": [
    "nb_class = len(np.unique(y))\n",
    "print('nb_class', nb_class)\n",
    "hidden_dim = 160\n",
    "best_weights_filepath = '../models/best_weights.hdf5'\n",
    "input_shape = X.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import LSTM\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import metrics\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.engine import Model\n",
    "from keras import optimizers\n",
    "\n",
    "def build_model():\n",
    "    image_input = Input(shape=input_shape)\n",
    "    \n",
    "    conv1 = Conv2D(20, (4, 4), name='conv1')(image_input)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2), name='pool1')(conv1)\n",
    "#     pool1 = Dropout(rate=0.2)(pool1)\n",
    "    \n",
    "    conv2 = Conv2D(40, (3, 3), name='conv2')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2), name='pool2')(conv2)\n",
    "#     pool2 = Dropout(rate=0.2)(pool2)\n",
    "\n",
    "    conv3 = Conv2D(60, (3, 3), name='conv3')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2), name='pool3')(conv3)\n",
    "\n",
    "    flat1 = Flatten(name='flat1')(pool3)\n",
    "    \n",
    "    conv4 = Conv2D(80, (2, 2), name='conv4')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    flat2 = Flatten(name='flat2')(conv4)\n",
    "    \n",
    "    merged = concatenate([flat1, flat2])\n",
    "    \n",
    "    out = Dense(hidden_dim, name='hidden1')(merged)\n",
    "    out = BatchNormalization()(out)\n",
    "    out = Activation('relu',name='deepid')(out)\n",
    "    out = Dense(nb_class, activation='softmax', name='softmax_class')(out)\n",
    "    \n",
    "    model = Model(inputs=image_input, outputs=out)\n",
    "\n",
    "    # optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0001)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam', #rmsprop\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 55, 47, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 52, 44, 20)    340                                          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 52, 44, 20)    80                                           \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 52, 44, 20)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)             (None, 26, 22, 20)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                   (None, 24, 20, 40)    7240                                         \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 24, 20, 40)    160                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 24, 20, 40)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)             (None, 12, 10, 40)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                   (None, 10, 8, 60)     21660                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 10, 8, 60)     240                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 10, 8, 60)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)             (None, 5, 4, 60)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                   (None, 4, 3, 80)      19280                                        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 4, 3, 80)      320                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 4, 3, 80)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flat1 (Flatten)                  (None, 1200)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flat2 (Flatten)                  (None, 960)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 2160)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "hidden1 (Dense)                  (None, 160)           345760                                       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 160)           640                                          \n",
      "____________________________________________________________________________________________________\n",
      "deepid (Activation)              (None, 160)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "softmax_class (Dense)            (None, 19433)         3128713                                      \n",
      "====================================================================================================\n",
      "Total params: 3,524,433.0\n",
      "Trainable params: 3,523,713.0\n",
      "Non-trainable params: 720.0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1067833 samples, validate on 118649 samples\n",
      "Epoch 1/40\n",
      "178s - loss: 6.9981 - acc: 0.0896 - val_loss: 5.5060 - val_acc: 0.1683\n",
      "Epoch 2/40\n",
      "175s - loss: 3.9151 - acc: 0.3603 - val_loss: 3.7649 - val_acc: 0.3838\n",
      "Epoch 3/40\n",
      "177s - loss: 2.8266 - acc: 0.5105 - val_loss: 3.2534 - val_acc: 0.4544\n",
      "Epoch 4/40\n",
      "179s - loss: 2.3047 - acc: 0.5878 - val_loss: 3.0757 - val_acc: 0.4826\n",
      "Epoch 5/40\n",
      "181s - loss: 1.9785 - acc: 0.6380 - val_loss: 2.9288 - val_acc: 0.5043\n",
      "Epoch 6/40\n",
      "183s - loss: 1.7489 - acc: 0.6738 - val_loss: 2.8019 - val_acc: 0.5237\n",
      "Epoch 7/40\n",
      "181s - loss: 1.5773 - acc: 0.7019 - val_loss: 2.8010 - val_acc: 0.5275\n",
      "Epoch 8/40\n",
      "181s - loss: 1.4480 - acc: 0.7217 - val_loss: 2.6829 - val_acc: 0.5413\n",
      "Epoch 9/40\n",
      "182s - loss: 1.3413 - acc: 0.7390 - val_loss: 2.6706 - val_acc: 0.5461\n",
      "Epoch 10/40\n",
      "182s - loss: 1.2547 - acc: 0.7536 - val_loss: 2.6645 - val_acc: 0.5501\n",
      "Epoch 11/40\n",
      "182s - loss: 1.1819 - acc: 0.7655 - val_loss: 2.6183 - val_acc: 0.5621\n",
      "Epoch 12/40\n",
      "180s - loss: 1.1219 - acc: 0.7758 - val_loss: 2.5906 - val_acc: 0.5623\n",
      "Epoch 13/40\n",
      "180s - loss: 1.0704 - acc: 0.7843 - val_loss: 2.5359 - val_acc: 0.5757\n",
      "Epoch 14/40\n",
      "182s - loss: 1.0255 - acc: 0.7919 - val_loss: 2.5751 - val_acc: 0.5703\n",
      "Epoch 15/40\n",
      "180s - loss: 0.9820 - acc: 0.7995 - val_loss: 2.5549 - val_acc: 0.5756\n",
      "Epoch 16/40\n",
      "181s - loss: 0.9476 - acc: 0.8050 - val_loss: 2.6745 - val_acc: 0.5623\n",
      "Epoch 17/40\n",
      "183s - loss: 0.9151 - acc: 0.8108 - val_loss: 2.5952 - val_acc: 0.5734\n",
      "Epoch 18/40\n",
      "181s - loss: 0.8887 - acc: 0.8150 - val_loss: 2.7762 - val_acc: 0.5586\n",
      "Epoch 19/40\n",
      "181s - loss: 0.8618 - acc: 0.8203 - val_loss: 2.6528 - val_acc: 0.5720\n",
      "Epoch 20/40\n",
      "182s - loss: 0.8374 - acc: 0.8246 - val_loss: 2.6971 - val_acc: 0.5664\n",
      "Epoch 21/40\n",
      "182s - loss: 0.8141 - acc: 0.8290 - val_loss: 2.6375 - val_acc: 0.5763\n",
      "Epoch 22/40\n",
      "182s - loss: 0.7945 - acc: 0.8321 - val_loss: 2.6123 - val_acc: 0.5843\n",
      "Epoch 23/40\n",
      "181s - loss: 0.7763 - acc: 0.8356 - val_loss: 2.6998 - val_acc: 0.5704\n",
      "Epoch 24/40\n",
      "180s - loss: 0.7580 - acc: 0.8390 - val_loss: 2.6020 - val_acc: 0.5879\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8986ff8cf8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveBestModel = keras.callbacks.ModelCheckpoint(\n",
    "    best_weights_filepath, \n",
    "    monitor='val_loss', \n",
    "    verbose=0, \n",
    "    save_best_only=True, \n",
    "    mode='auto'\n",
    ")\n",
    "earlyStopping=keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    verbose=1, \n",
    "    mode='auto'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    batch_size=1024, \n",
    "    epochs=40,\n",
    "    verbose=2, \n",
    "    validation_split=0.1, \n",
    "    shuffle=True,\n",
    "    callbacks=[saveBestModel, earlyStopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(best_weights_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('../models/webface-full-celeba-simple-cnn.aligned.margin42.grey.flipped.s40.model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.5350472283700447, 0.5769691729337254]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=1024, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
